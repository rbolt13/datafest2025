---
title: "structuredtopicmodelingwithtaylor"
format: html
draft: true
execute: 
  eval: false
---

```{r}
# install.packages("taylor")
# install.packages("stm")
# install.packages(c("tm", "SnowballC"))
# install.packages("tidytext")
# install.packages("reshape2")
library("reshape2")
library(tidytext)
library("taylor")
library(stm)
library(ggplot2)
library(tidyr)
library(purrr)
set.seed(123)
```



# Define Data 


```{r}
Data <- data.frame(
  doc = 1:nrow(taylor_all_songs),
  track = taylor_all_songs$track_name,
  album = taylor_all_songs$album_name, 
  date = taylor_all_songs$track_release,
  lyrics = NA_character_,
  mode = taylor_all_songs$mode_name,
  danceability = taylor_all_songs$danceability
)

for (i in 1:nrow(taylor_all_songs)) {
  test <- data.frame(taylor_all_songs$lyrics[i])
  test <- test[test$element_artist == "Taylor Swift", ]
  Data$lyrics[i] <- paste(unlist(test$lyric), collapse = " ")
}

Data_no_tv <- subset(Data, !grepl("\\(Taylor's Version\\)|\\[Taylor's Version\\]|\\(Acoustic Version\\)|\\(Pop Version\\)|\\(Piano Version\\)|\\(Remix\\)", track, ignore.case = TRUE))

Data_with_tv <- Data
Data <- Data_no_tv
```

```{r}
# helper to normalize a line for duplicate checking
norm_line <- function(s) {
  s <- tolower(trimws(s))
  s <- gsub("[^a-z0-9\\s]", "", s)  # drop punctuation
  gsub("\\s+", " ", s)              # squeeze spaces
}

Data <- data.frame(
  doc = 1:nrow(taylor_all_songs),
  track = taylor_all_songs$track_name,
  album = taylor_all_songs$album_name, 
  date = taylor_all_songs$track_release,
  lyrics = NA_character_,
  mode = taylor_all_songs$mode_name,
  danceability = taylor_all_songs$danceability
)

for (i in 1:nrow(taylor_all_songs)) {
  test <- data.frame(taylor_all_songs$lyrics[i])
  test <- test[test$element_artist == "Taylor Swift", ]

  if (nrow(test) == 0) {
    Data$lyrics[i] <- NA_character_
  } else {
    # drop any repeated lines within the song (keep first occurrence)
    keep <- !duplicated(norm_line(test$lyric))
    test <- test[keep, , drop = FALSE]
    Data$lyrics[i] <- paste(test$lyric, collapse = " ")
  }
}

Data_no_tv <- subset(Data, !grepl("\\(Taylor's Version\\)|\\[Taylor's Version\\]", track, ignore.case = TRUE))

Data_with_tv <- Data
Data <- Data_no_tv

```


```{r}
# --- CLEAN ---
processed <- textProcessor(
  documents        = Data$lyrics,
  metadata         = Data,
  lowercase        = TRUE,
  removestopwords  = TRUE,
  removenumbers    = FALSE,   # keep numbers
  removepunctuation= TRUE,
  stem             = FALSE,
  wordLengths      = c(3, Inf)
)

out <- prepDocuments(
  processed$documents,
  processed$vocab,
  processed$meta,
  lower.thresh = 5
)
docs  <- out$documents
vocab <- out$vocab
meta  <- out$meta

# keep rows that have all covariates present (since we'll use them in prevalence)
covars <- c("album", "mode", "danceability")
keep   <- complete.cases(meta[, covars, drop = FALSE])
docs   <- docs[keep]
meta   <- droplevels(meta[keep, , drop = FALSE])
# vocab stays as-is
```

```{r}
k_grid <- c(7, 8, 9, 10, 11, 12, 13)

k_result <- searchK(
  documents  = docs,
  vocab      = vocab,
  data       = meta,
  K          = k_grid,
  prevalence = ~ album + mode + danceability,
  init.type  = "Spectral",
  max.em.its = 75
)
```

```{r}
res <- k_result$results

res$K        <- as.integer(unlist(res$K))

# make list-cols numeric
res$heldout  <- unlist(res$heldout)
res$residual <- unlist(res$residual)
res$semcoh   <- sapply(res$semcoh, mean,  na.rm = TRUE)
res$exclus   <- sapply(res$exclus,  mean,  na.rm = TRUE)
res$bound   <- unlist(res$bound)
res$lbound  <- sapply(res$lbound, tail, n = 1) 
res$em.its  <- unlist(res$em.its)

# now view just K and the key diagnostics
res[, c("K", "heldout", "residual", "semcoh", "exclus")]


res
```



```{r}
plot(res)
```


```{r}
res %>%
  select(K, exclus, semcoh) %>%
  unnest() %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semcoh, exclus, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```

```{r}
fit <- stm(
  documents = docs,
  vocab     = vocab,
  K         = 10, 
  data      = meta,
  prevalence= ~ album + mode,
  init.type = "Spectral"
)
```


```{r}
# library(dplyr)
# library(tidytext)
# library(ggplot2)
# install.packages("ggthemes")
# library(ggthemes)
# library(scales)

# Tidy matrices from your STM fit
td_beta  <- tidy(fit, matrix = "beta")   # columns: topic, term, beta
td_gamma <- tidy(fit, matrix = "gamma")  # columns: document, topic, gamma

# Top 7 terms per topic (by beta)
top_terms <- td_beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 7, with_ties = FALSE) %>%
  arrange(topic, desc(beta)) %>%
  summarise(terms = paste(term, collapse = ", "), .groups = "drop")

# Mean topic prevalence (gamma) and attach top terms
gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma), .groups = "drop") %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = reorder(paste0("Topic ", topic), gamma))

# Plot all topics (or change n to limit)
n_topics <- n_distinct(gamma_terms$topic)

gamma_terms %>%
  slice_max(gamma, n = n_topics, with_ties = FALSE) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, max(gamma_terms$gamma) * 1.1),
                     labels = percent_format()) +
  theme_tufte(ticks = FALSE) +
  labs(x = NULL, y = expression(gamma),
       title = "Topic prevalence in Taylor Swift songs",
       subtitle = "Top 7 words per topic (by probability)")

```

```{r}
extra_sw <- c("oh","yeah","ooh","na","la","mm","hmm","whoa","hey","uh","oh-oh","gonna","wanna","gotta")

processed2 <- textProcessor(
  documents        = Data$lyrics,
  metadata         = Data,
  lowercase        = TRUE,
  removestopwords  = TRUE,
  removenumbers    = TRUE,
  removepunctuation= TRUE,
  stem             = FALSE,                 # <- turn off stemming for lyrics
  customstopwords  = extra_sw,
  wordLengths      = c(3, Inf)
)

out2 <- prepDocuments(processed2$documents, processed2$vocab, processed2$meta, lower.thresh = 8)

set.seed(123)
fit_simple <- stm(out2$documents, out2$vocab, K = 10, init.type = "Spectral")

# then label with FREX again:
labelTopics(fit_simple, n = 10)

```




```{r}


gamma_terms |> select(topic, gamma, terms) |> arrange(desc(gamma))
```


```{r}
library(dplyr)


gamma <- tidy(fit, matrix = "gamma")             # document-topic weights
meta2 <- meta |> mutate(document = row_number())# same meta used to fit

reps <- gamma %>%
  group_by(topic) %>%
  slice_max(gamma, n = 5, with_ties = FALSE) %>%
  ungroup() %>%
  left_join(meta2, by = "document") %>%
  transmute(topic, track, album, date, weight = round(gamma, 3)) %>%
  arrange(topic, desc(weight))

reps

```






```{r}
library(dplyr)
library(tidytext)
library(ggplot2)
library(scales)

# doc–topic + album
td_gamma <- tidy(fit, matrix = "gamma")
meta2    <- meta %>% mutate(document = row_number())

by_album <- td_gamma %>%
  left_join(meta2[, c("document","album")], by = "document") %>%
  group_by(album, topic) %>%
  summarise(mean_gamma = mean(gamma), .groups = "drop")

ggplot(by_album, aes(x = factor(topic), y = album, fill = mean_gamma)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(labels = percent_format(), option = "C") +
  labs(x = "Topic", y = "Album", fill = "Mean share",
       title = "Topic prevalence by album") +
  theme_minimal() +
  theme(panel.grid = element_blank())


```

```{r}
library(dplyr); library(tidyr); library(tidytext); library(ggplot2); library(scales)

td_gamma <- tidy(fit, matrix = "gamma")
meta2    <- meta %>% mutate(document = row_number())

by_mode <- td_gamma %>%
  left_join(meta2[, c("document","mode")], by = "document") %>%
  group_by(mode, topic) %>%
  summarise(mean_gamma = mean(gamma), .groups = "drop")

ggplot(by_mode, aes(x = factor(topic), y = mode, fill = mean_gamma)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(labels = percent_format(), option = "C") +
  labs(x = "Topic", y = "Mode", fill = "Mean share",
       title = "Topic prevalence by musical mode")

```

# No Covariates

```{r}
# add the junk you’re seeing + common lyric filler/contractions
lyric_sw <- c(
  "dont","doesnt","didnt","cant","couldnt","wont","wouldnt","isnt","arent","aint",
  "im","ive","id","ill","youre","youll","youd","shes","hes","theyre","weve","were","youve",
  "cause","cuz","coz","yeah","oh","ooh","ah","uh","mm","hmm","hey","whoa","na","la","ohoh","oooh",
  "gonna","wanna","gotta","til","till","just","kinda","sorta",
  # very generic verbs you said you want out:
  "come","get","got","see","make","say","know","need","want","time","one","back","like","cant","dont","youre"
)

processed2 <- textProcessor(
  documents        = Data$lyrics,
  metadata         = Data,
  lowercase        = TRUE,
  removestopwords  = TRUE,
  customstopwords  = lyric_sw,
  removenumbers    = TRUE,
  removepunctuation= TRUE,
  stem             = FALSE,
  wordLengths      = c(3, Inf)
)

out2 <- prepDocuments(processed2$documents, processed2$vocab, processed2$meta,
                      lower.thresh = 10)

docs  <- out2$documents
vocab <- out2$vocab
meta  <- out2$meta

set.seed(123)
fit <- stm(docs, vocab, K = 6, init.type = "Spectral")

```

```{r}
td_beta  <- tidy(fit, matrix = "beta")   # columns: topic, term, beta
td_gamma <- tidy(fit, matrix = "gamma")  # columns: document, topic, gamma

# Top 7 terms per topic (by beta)
top_terms <- td_beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 7, with_ties = FALSE) %>%
  arrange(topic, desc(beta)) %>%
  summarise(terms = paste(term, collapse = ", "), .groups = "drop")

# Mean topic prevalence (gamma) and attach top terms
gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma), .groups = "drop") %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = reorder(paste0("Topic ", topic), gamma))

# Plot all topics (or change n to limit)
n_topics <- n_distinct(gamma_terms$topic)

library(ggplot2)
library(scales)
library(ggthemes)

gamma_terms %>%
  slice_max(gamma, n = n_topics, with_ties = FALSE) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3) +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, max(gamma_terms$gamma) * 1.1),
                     labels = percent_format()) +
  theme_tufte(ticks = FALSE) +
  labs(x = NULL, y = expression(gamma),
       title = "Topic prevalence in Taylor Swift songs",
       subtitle = "Top 7 words per topic (by probability)")

```



```{r}
gamma <- tidy(fit, matrix = "gamma")             # document-topic weights
meta2 <- meta |> mutate(document = row_number())# same meta used to fit

reps <- gamma %>%
  group_by(topic) %>%
  slice_max(gamma, n = 5, with_ties = FALSE) %>%
  ungroup() %>%
  left_join(meta2, by = "document") %>%
  transmute(topic, track, album, date, weight = round(gamma, 3)) %>%
  arrange(topic, desc(weight))

reps
```

# Word Cloud

```{r}
install.packages(c("wordcloud","RColorBrewer"))  # if not installed
library(tidytext)
library(dplyr)
library(wordcloud)
library(RColorBrewer)

```


```{r}
#| warning: false
#| message: false
td_beta <- tidy(fit, matrix = "beta")
K <- ncol(fit$theta)

par(mfrow = c(ceiling(K/3), 3), mar = c(1,1,2,1))  # tweak rows/cols if needed
for (k in 1:K) {
  df <- td_beta %>%
    filter(topic == k) %>%
    slice_max(beta, n = 100)

  wordcloud(words = df$term, freq = df$beta,
            max.words = 80, random.order = FALSE,
            scale = c(3, 0.7), colors = brewer.pal(8, "Dark2"))
  title(paste("Topic", k), line = -1)
}
par(mfrow = c(1,1))
```

# bigrams


```{r}
# Bigram STM: clean → bigrams → prep → fit (one chunk)

library(dplyr)
library(tidyr)
library(tidytext)
library(stm)

## 1) Assemble per-song lyrics (Taylor-only), drop TV/alt versions
Data <- data.frame(
  doc   = 1:nrow(taylor_all_songs),
  track = taylor_all_songs$track_name,
  album = taylor_all_songs$album_name,
  date  = taylor_all_songs$track_release,
  lyrics= NA_character_,
  mode  = taylor_all_songs$mode_name,
  danceability = taylor_all_songs$danceability
)

for (i in 1:nrow(taylor_all_songs)) {
  test <- data.frame(taylor_all_songs$lyrics[i])
  test <- test[test$element_artist == "Taylor Swift", ]
  Data$lyrics[i] <- paste(unlist(test$lyric), collapse = " ")
}

pattern <- "\\(Taylor's Version\\)|\\[Taylor's Version\\]|\\(Acoustic Version\\)|\\(Pop Version\\)|\\(Piano Version\\)|\\(Remix\\)"
Data <- subset(Data, !grepl(pattern, track, ignore.case = TRUE))

## 2) Replace lyrics with BIGRAM text (stopwords removed, tokens joined by "_")
sw <- stop_words$word

bigrams_text <- Data %>%
  select(doc, lyrics) %>%
  mutate(lyrics = tolower(lyrics)) %>%
  unnest_tokens(bigram, lyrics, token = "ngrams", n = 2) %>%
  separate(bigram, c("w1","w2"), sep = " ", remove = TRUE) %>%
  filter(!(w1 %in% sw | w2 %in% sw)) %>%
  transmute(doc, tok = paste(w1, w2, sep = "_")) %>%
  group_by(doc) %>%
  summarise(lyrics = paste(tok, collapse = " "), .groups = "drop")

Data <- Data %>% select(-lyrics) %>% left_join(bigrams_text, by = "doc") %>%
  filter(!is.na(lyrics) & nzchar(lyrics))

## 3) STM processing (keep underscores; no extra stopword/stem)
processed <- textProcessor(
  documents        = Data$lyrics,
  metadata         = Data,
  lowercase        = FALSE,
  removestopwords  = FALSE,
  removenumbers    = FALSE,
  removepunctuation= FALSE,
  stem             = FALSE,
  wordLengths      = c(3, Inf)
)

out <- prepDocuments(
  processed$documents,
  processed$vocab,
  processed$meta,
  lower.thresh = 5
)

docs  <- out$documents
vocab <- out$vocab
meta  <- out$meta

## 4) Fit STM (choose K safely below vocab size)
set.seed(123)
K <- max(3, min(8, length(vocab) - 1L))
fit <- stm(docs, vocab, K = K, init.type = "Spectral")

```


