---
title: "2025 Data Fest"
subtitle: "Introduction to R, Text Mining, and Sentiment Analysis"
author: "[Randi Bolt](https://randibolt.com/)"
date: "9/25/2025"
format:
  revealjs:
    theme: serif
    logo: img/bolt.png
    footer: "[2025-datafest.netlify.app](https://2025-datafest.netlify.app/#/title-slide)"
    incremental: true 
---

## About Me

I’m the Data Equity and Innovation Supervisor for **[Paid Leave Oregon](https://paidleave.oregon.gov/)**, where I lead a team of data analysts in transforming complex data into actionable insights.

<br>

Outside of work, I explore data through various personal projects that incorporate analytics, visualization, and storytelling. Check out my [blogs](https://2024-rbolt.netlify.app/projects/technical-blogs), [dashboards](https://randibolt.com/dashboards), and [talks](https://randibolt.com/talks) to see more.

## Agenda

* Introduction to R & R Studio

* Taylor Data

* Text Mining

* Sentiment Analysis

## Introduction to R & R Studio 

{{< video src="https://www.youtube.com/watch?v=wIft-t-MQuE" width="100%" height="80%" >}}

## What is R

R is a statistical programming language that's incredibly powerful for working with data. 

<br>

Unlike Python which is built around _objects_, R is based on _functions_. For our purposes, that means we'll be calling functions (like $F(x)=Y$) to transform our data rather than attaching properties to objects. 


## Benefits of R

* R is open source and freely available. 

* R has an extensive and coherent set of tools for statistical analysis. 

* R has an extensive and highly flexible graphical facility capable of production publication quality figures. 

* R has an extensive support network with numerous online and freely available documents. 

* R has an expanding set of freely available 'packages' to extend R's capabilities. [^1]

[^1]: [Intro to R](https://intro2r.com/chap1.html)

## What is R Studio 

R Stuido is an **I**ntegrated **D**evelopment **E**nvironment (IDE), similar to VS Code or Sublime. 

<br>

R Studio provides a more user-friendly interface, incorporating the R Console, a script editor and other useful functionality (like R markdown and GitHub integration). You can find more information about [RStudio here](). [^2]

[^2]: [Intro to R](https://intro2r.com/chap1.html)

## Live Demo

* Landscape (Console, help, viewer, environment, files)

* Global Options 

* R Syntax

## Packages 

Most of the work we'll do relies on packages, which are basically toolkits. To use one, you will first need to install it with a base R function `install.packages()`: 


```{r}
#| echo: true
#| eval: false
install.packages("package_name")
```

After installing a package, you can load it into your current session with `library()`:

```{r}
#| echo: true
#| eval: false
library("package_name")
```

## Tips

When in doubt about what a functions does, or what is in a package, you can type `?function_name()` or `?package_name` in the R Studio console to open the description page in the Help tab. 

## CRAN

CRAN is the Comprehensive R Archive Network. It’s where R packages are stored, tested, and shared with the community. If you install a package in R, you’re usually pulling it from CRAN.

## Taylor Swift Data

{{< video src="https://www.youtube.com/watch?v=e-ORhEE9VVg" width="100%" height="80%" >}}

## Taylor Package

The Taylor package is a comprehensive resource for data on Taylor Swift songs. Data comes from 'Genius' (lyrics) and 'Spotify' (song characteristics). 

**Useful links**: [taylor](https://taylor.wjakethompson.com/), [taylor repo](https://github.com/wjakethompson/taylor)


### Install & Load Data

```{r}
#| warning: false
#| error: false
install.packages("taylor", repos="http://cran.us.r-project.org")
library("taylor")
```

```{r}
#| echo: true
#| eval: false
install.packages("taylor")
library("taylor")
```

## Taylor Data

There are three main data sets: `taylor_album_songs`

```{r}
#| echo: true
head(taylor_album_songs)
```

## Reactable

This package creates a data table with sorting and pagination. The default table is an HTML widget that can be used in RMD and Shiny applications, or viewed from an R console. 

```{r}
#| warning: false
#| error: false
install.packages("reactable", repos="http://cran.us.r-project.org")
library("reactable")
```

```{r}
#| echo: true
#| eval: false
install.packages("reactable")
library("reactable")
```

## Reactable Taylor Data

```{r}
#| echo: true
reactable(
  taylor_album_songs,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## Extract Lyrics

Lets look at one song: 

```{r}
song1 <- data.frame(taylor_all_songs$lyrics[1])
```

```{r}
#| echo: true
#| eval: false
song1 <- data.frame(taylor_all_songs$lyrics[1])
head(song1)
```

```{r}
reactable(
  song1,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## Introduction to Text Mining

{{< video src="https://www.youtube.com/watch?v=b1kbLwvqugk" width="100%" height="80%" >}}

## Key Definitions

These definitions might feel elementary at first, but that's the point. The more clearly you understand these simple ideas, the easier it will be to make sense of the more complex modeling steps later. 

* **Word**: a single word, and smallest unit of analysis.

* **Text**: the written content inside a document (the lyrics of a single song). 

* **Document**: a unit of text (a single song). 

* **Corpus**: the full collection of texts (all song lyrics). 

## Key Definitions Continued

* **Vocabulary**: the unique set of words across all documents in a set. 

* **N-grams**: sequence of words (1 = unigram, 2 = bigram, 3 = trigram), we we can use to look for corpus themes. 

* **Stop Words**: Stop words are common words (like the, and, is, in, at, on) that usually don’t add much meaning for text analysis.

* **Sentiment**: Sentiment refers to the emotional tone or attitude expressed in text.

## Create Data 

```{r}
#| echo: true
Data <- data.frame(
  doc = 1:nrow(taylor_all_songs),
  track = taylor_all_songs$track_name,
  album = taylor_all_songs$album_name, 
  lyrics = NA_character_
)

for (i in 1:nrow(taylor_all_songs)) {
  test <- data.frame(taylor_all_songs$lyrics[i])
  test <- test[test$element_artist == "Taylor Swift", ]
  Data$lyrics[i] <- paste(unlist(test$lyric), collapse = " ")
}
```

## Remove Taylor's Versions

(text ...)

```{r}
#| echo: true
Data_no_tv <- subset(
  Data, 
  !grepl(
    "\\(Taylor's Version\\)
    |\\[Taylor's Version\\]
    |\\(Acoustic Version\\)
    |\\(Pop Version\\)
    |\\(Piano Version\\)
    |\\(Remix\\)", 
    track, 
    ignore.case = TRUE))

Data_with_tv <- Data
Data <- Data_no_tv
```

## Dplyr & Tidytext

(add text)

```{r}
#| echo: true
#| eval: false
install.packages("dplyr")
install.packages("tidytext")
install.packages("tidyr")
library(dplyr)
library(tidytext)
library(tidyr)
data("stop_words")
song_words <- c(
  "ooh", "di", "eh", "ah", "la", "ha",
  "da", "uh", "huh", "whoa")
```

```{r}
library(dplyr)
library(tidytext)
library(tidyr)
data("stop_words")
song_words <- data.frame(
  word = c(
    "ooh", "di", "eh", "ah", 
    "la", "ha", "da", "uh", 
    "huh", "whoa", "mm"))
```

## Unigram

(add text)

## Unigram

```{r}
#| echo: true
unigram <- Data |>
  mutate(lyrics = tolower(lyrics)) |>
  unnest_tokens(word, lyrics, token = "words") |>
  anti_join(stop_words) |>
  anti_join(song_words)
```

```{r}
reactable(
  unigram,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## ggplot2

(add text.. live demo)

```{r}
#| echo: true
library(ggplot2)
```


## Most Common Word (code)

```{r}
#| echo: true
#| eval: false
unigram |>
  count(word, sort = TRUE) |>
  filter(n > 100) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

## Most Common Word (Graph)

```{r}
unigram %>%
  count(word, sort = TRUE) |>
  filter(n > 100) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) 
```

## Bigram (Code)

```{r}
bigram <- Data |>
  select(doc, lyrics) |>
  mutate(lyrics = tolower(lyrics)) |>
  unnest_tokens(bigram, 
                lyrics, 
                token = "ngrams", 
                n = 2) |>
  separate(
    col = bigram,
    sep = " ",
    into = c("w1", "w2"),
    remove = FALSE
  ) |>
  filter(!w1 %in% stop_words$word) |>
  filter(!w2 %in% stop_words$word) |>
  filter(!w1 %in% song_words$word) |>
  filter(!w2 %in% song_words$word)
```

```{r}
#| echo: true
#| eval: false
bigram <- Data |>
  select(doc, lyrics) |>
  mutate(lyrics = tolower(lyrics)) |>
  unnest_tokens(
    bigram, 
    lyrics, 
    token = "ngrams", 
    n = 2) |>
  separate(
    col = bigram,
    sep = " ",
    into = c("w1", "w2"),
    remove = FALSE
  ) |>
  filter(!w1 %in% stop_words$word) |>
  filter(!w2 %in% stop_words$word) |>
  filter(!w1 %in% song_words$word) |>
  filter(!w2 %in% song_words$word)
```

## Bigram (Table)

```{r}
reactable(
  bigram,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## Most Common Bigram (Code)

```{r}
#| echo: true
#| eval: false
bigram |>
  count(bigram, sort = TRUE) |>
  filter(n > 13) |>
  mutate(bigram = reorder(bigram, n)) |>
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL)
```

## Most Common Bigram (Graph)

```{r}
bigram |>
  count(bigram, sort = TRUE) |>
  filter(n > 13) |>
  mutate(bigram = reorder(bigram, n)) |>
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL)
```

## Sentiment Analysis 

{{< video src="https://youtu.be/K-a8s8OLBSE?si=YKbF1sMcnuTm_7Mj" width="100%" height="80%" >}}

## Sentiment Dictionaries

The tidytext package provides access to several sentiment lexicons. 

```{r}
#| warning: false
#| error: false
install.packages("tidytext", repos="http://cran.us.r-project.org")
library("tidytext")
```

```{r}
#| echo: true
#| eval: false
install.packages("tidytext")
library("tidytext")
```


```{r}
# install.packages("textdata", repos="http://cran.us.r-project.org")
# library("textdata")
```

There are three general-purpose lexicons are included wiht tidytext. We will be using two. 

## Lixicons - Bing

**[bing](https://search.r-project.org/CRAN/refmans/textdata/html/lexicon_bing.html)**: the words are assigned scores for positive/negative sentiment. 

```{r}
#| echo: true
#| eval: false
get_sentiments("bing")
```

```{r}
get_sentiments("bing")
```


## Lexicons - Afinn

**[afinn](https://search.r-project.org/CRAN/refmans/oolong/html/afinn.html)**: assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

```{r}
#| echo: true
#| eval: false
get_sentiments("afinn")
```

```{r}
get_sentiments("afinn")
```

## Important Note

These dictionaries were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data. Given this information, we may hesitate to apply these sentiment lexicons to styles of text dramatically different from what they were validated on. [^2]

[^2]: [Text Mining with R - Sentiment analysis with tidy data](https://www.tidytextmining.com/sentiment#the-sentiments-datasets)


## Tidy Albums

```{r}
# tidy_albums <- Data |>
#   group_by(album) |>
#   mutate(
#     track = row_number() 
#   ) |>
#   ungroup() |>
#   unnest_tokens(word, text)
```
## Cont.. 


(Cont)

* over time by albumn

## Bing Join

```{r}
bing_word_counts <- unigram |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  ungroup()
```

## Top + and - Words


```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

## Word cloud

```{r}
library(wordcloud)
library(RColorBrewer)

unigram %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

## Review

(things to revieq)


## Conclusion 

Good bye thanks

