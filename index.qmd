---
title: "2025 Data Fest"
subtitle: "Introduction to R, Text Mining, and Sentiment Analysis"
author: "[Randi Bolt](https://randibolt.com/)"
date: "9/23/2025"
format:
  revealjs:
    theme: serif
    logo: img/bolt.png
    footer: "[2025-datafest.netlify.app](https://2025-datafest.netlify.app/#/title-slide)"
    incremental: true 
---

## About Me

I’m the Data Equity and Innovation Supervisor for **[Paid Leave Oregon](https://paidleave.oregon.gov/)**, where I lead a team of data analysts in transforming complex data into actionable insights.

<br>

Outside of work, I explore data through various personal projects that incorporate analytics, visualization, and storytelling. Check out my [blogs](https://2024-rbolt.netlify.app/projects/technical-blogs), [dashboards](https://randibolt.com/dashboards), and [talks](https://randibolt.com/talks) to see more.

## Agenda

* Introduction to R & R Studio

* Taylor Data

* Text Mining

* Sentiment Analysis

## Introduction to R & R Studio 

{{< video src="https://www.youtube.com/watch?v=wIft-t-MQuE" width="100%" height="80%" >}}

## What is R

R is a statistical programming language that's incredibly powerful for working with data. 

<br>

Unlike Python which is built around _objects_, R is based on _functions_. For our purposes, that means we'll be calling functions (like $F(x)=Y$) to transform our data rather than attaching properties to objects. 


## Benefits of R

* R is open source and freely available. 

* R has an extensive and coherent set of tools for statistical analysis. 

* R has an extensive and highly flexible graphical facility capable of production publication quality figures. 

* R has an extensive support network with numerous online and freely available documents. 

* R has an expanding set of freely available 'packages' to extend R's capabilities. [^1]

[^1]: [Intro to R](https://intro2r.com/chap1.html)

## What is R Studio 

R Studio is an **I**ntegrated **D**evelopment **E**nvironment (IDE), similar to VS Code or Sublime. 

<br>

R Studio provides a more user-friendly interface, incorporating the R Console, a script editor and other useful functionality (like R markdown and GitHub integration). You can find more information about [RStudio here]().[^2]

[^2]: [Intro to R](https://intro2r.com/chap1.html)

## Packages 

Most of the work we'll do relies on packages, which are basically toolkits. To use one, you will first need to install it with a base R function `install.packages()`: 


```{r 1. install packages}
#| echo: true
#| eval: false
install.packages("package_name")
```

After installing a package, you can load it into your current session with `library()`:

```{r}
#| echo: true
#| eval: false
library("package_name")
```

## Tips

When in doubt about what a functions does, or what is in a package, you can type `?function_name()` or `?package_name` in the R Studio console to open the description page in the Help tab. 

## CRAN

CRAN is the Comprehensive R Archive Network. It’s where R packages are stored, tested, and shared with the community. If you install a package in R, you’re usually pulling it from CRAN.

## Live Demo

* Create and load R projects

* R Studio Landscape (Console, help, viewer, environment, files, packages)

* Global Options (appearance & layout)

* .R files, Quarto files, render code, comments

* R Syntax (assignment `<-`, concatenate `c()`, extract values `[]`, extract fields `$`, sequence `:`, loops)

* Plots & ggplots

## Review Questions

1. What is the difference between R and R Studio

2. How do you find documentation on a package or function? 

3. How do you find the version of R you are currently using? 

4. What is the assignment operator? 

## Taylor Swift Data

{{< video src="https://www.youtube.com/watch?v=e-ORhEE9VVg" width="100%" height="80%" >}}

## Taylor Package

The Taylor package is a comprehensive resource for data on Taylor Swift songs. Data comes from 'Genius' (lyrics) and 'Spotify' (song characteristics). 

**Useful links**: [taylor](https://taylor.wjakethompson.com/), [taylor repo](https://github.com/wjakethompson/taylor)


### Install & Load Data

```{r}
#| warning: false
#| error: false
install.packages("taylor", repos="http://cran.us.r-project.org")
library("taylor")
```

```{r}
#| echo: true
#| eval: false
install.packages("taylor")
library("taylor")
```

## Taylor Data

There are three main data sets: 

* `taylor_album_songs`: which includes lyrics and audio features from the Spotify API for all songs on Taylor’s official studio albums. 

* `taylor_all_songs`: Taylor’s entire discography. 

* `taylor_albums`: Sumarizes Taylor’s album release history.

## Taylor Album Songs

```{r}
#| echo: true
head(taylor_album_songs)
```

## Reactable

This package creates a data table with sorting and pagination. The default table is an HTML widget that can be used in RMD and Shiny applications, or viewed from an R console. 

```{r}
#| warning: false
#| error: false
install.packages("reactable", repos="http://cran.us.r-project.org")
library("reactable")
```

```{r}
#| echo: true
#| eval: false
install.packages("reactable")
library("reactable")
```

## Reactable Taylor Data

```{r}
#| echo: true
reactable(
  taylor_album_songs,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## Extract Lyrics

Lets look at one song: 

```{r}
song1 <- data.frame(taylor_all_songs$lyrics[1])
```

```{r}
#| echo: true
#| eval: false
song1 <- data.frame(taylor_all_songs$lyrics[1])
head(song1)
```

```{r}
reactable(
  song1,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## Dplyr

A package for working with data frames that makes it easy to filter, sort, group, and summarize data using simple, readable functions.

```{r}
install.packages("dplyr", repos="http://cran.us.r-project.org")
library(dplyr)
```

```{r}
#| echo: true
#| eval: false
install.packages("dplyr")
library(dplyr)
```

## Tidy Text

A package that helps turn text (like lyrics or survey responses) into tidy data frames, so you can analyze words, sentiments, and topics with the same tools you use for numbers.

```{r}
install.packages("tidytext", repos="http://cran.us.r-project.org")
library(tidytext)
```

```{r}
#| echo: true
#| eval: false
install.packages("tidytext")
library(tidytext)
```

## Tidyr

A package for reshaping data, used to make messy data “tidy” by separating, combining, or pivoting columns so each row is an observation and each column is a variable.

```{r}
install.packages("tidyr", repos="http://cran.us.r-project.org")
library(tidyr)
```

```{r}
#| echo: true
#| eval: false
install.packages("tidyr")
library(tidyr)
```

## Define Stop Words

A built-in dataset of very common words like the, and, of that usually don’t add much meaning. We remove these so the analysis focuses on more meaningful words.

```{r}
#| echo: true
data("stop_words")
```

```{r}
reactable(
  stop_words,
  wrap = FALSE,
  defaultPageSize = 3,
  defaultColDef = colDef(minWidth = 300))
```


## Define Songs Words

A custom list of filler or vocalization words (like ooh, ah, la) that appear in lyrics but don’t carry much meaning. We can filter these out so they don’t distract from the main analysis.

```{r}
#| echo: true
song_words <- data.frame(
  word = c(
    "ooh", "di", "eh", "ah", "la", 
    "ha","da", "uh", "huh", "whoa",
    "ba", "hoo")
)
```

## Define Negations

A dataset of words like not, no, never, without that flip the meaning of the words around them. This is useful in sentiment analysis because “not happy” is very different from “happy.”

```{r}
#| echo: true
negations <- data.frame(
  word = c(
      "not", "no", "never", "none", "nobody",
      "nothing", "neither", "nowhere", "cannot",
      "without", "hardly", "barely", "scarcely")
)
```

## Define Data

```{r}
#| echo: true
Data <- lapply(1:nrow(taylor_album_songs), function(i) {
  song  <- taylor_album_songs[i, ]
  lines <- song$lyrics[[1]]

  data.frame(
    doc         = i,
    track       = song$track_name,
    album       = song$album_name,
    line_number = seq_len(nrow(lines)),
    lyrics       = lines$lyric
  )
}) |> 
  bind_rows()
```

## Review Data

```{r}
reactable(
  Data,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## Review Questions

1. What is the importance of stop words?

2. Why are negations important to consider when analyzing sentiment?

3. What does the doc column represent in the dataset we created from Taylor’s lyrics?

4. Why might we want to keep each line of a song instead of collapsing the whole song into one string?

## Introduction to Text Mining

{{< video src="https://www.youtube.com/watch?v=b1kbLwvqugk" width="100%" height="80%" >}}

## Key Definitions

These definitions might feel elementary at first, but that's the point. The more clearly you understand these simple ideas, the easier it will be to make sense of the more complex modeling steps later. 

* **Word**: a single word, and smallest unit of analysis.

* **Text**: the written content inside a document (the lyrics of a single song). 

* **Document**: a unit of text (a single song). 

* **Corpus**: the full collection of texts (all song lyrics). 

## Key Definitions Continued

* **Vocabulary**: the unique set of words across all documents in a set. 

* **N-grams**: sequence of words (1 = unigram, 2 = bigram, 3 = trigram), we we can use to look for corpus themes. 

* **Stop Words**: Stop words are common words (like the, and, is, in, at, on) that usually don’t add much meaning for text analysis.

* **Sentiment**: Sentiment refers to the emotional tone or attitude expressed in text.

## Unnest Tokens

This splits a column into tokens, flattening the table into one-token-per-row. 

Let's start by tokenizing our data set, keeping stop words and song words. 

```{r}
#| echo: true
unigram_all <- Data |>
  unnest_tokens(word, lyrics, token = "words") 
```

## Review unigram_all

Notice that there are 132,140 total words in our Corpus. 

```{r}
reactable(
  unigram_all,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## Vocabulary (unigram_all)

When we count the words we see that there are only 5,146 words in this vocabulary, with the most common words being you, I, the, and and. 

```{r}
#| echo: true
unigram_all_vocab <- unigram_all |>
  count(word, sort = TRUE)
```

```{r}
reactable(
  unigram_all_vocab,
  wrap = FALSE,
  defaultPageSize = 2,
  defaultColDef = colDef(minWidth = 300))
```


## Ggplot

To visualize the most common words in our vocabulary we are going to use the `ggplot2` package. 

```{r}
#| echo: true
#| eval: false
install.packages("ggplot2")
library("ggplot2")
```

```{r}
install.packages("ggplot2", repos="http://cran.us.r-project.org")
library("ggplot2")
```

## Visualize Plot

Then we will use the following code to visualize the most used words in our vocabulary. 

```{r}
#| echo: true
#| eval: false
unigram_all_vocab |>
  filter(n > 1000) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

## Most Used Words (all)

```{r}
unigram_all_vocab |>
  filter(n > 1000) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

## Removing Stop & Song Words

Since we are not able to derive much insight from that initial analysis, lets try again but removing stop words and our custom song words data frames using an `anti_join`. 

```{r}
#| echo: true
unigram <- Data |>
  unnest_tokens(word, lyrics, token = "words") |>
  anti_join(stop_words) |>
  anti_join(song_words)
```

## Anti Join Unigram

```{r}
reactable(
  unigram,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```


## Most Common Word (Code)

```{r}
#| echo: true
#| eval: false
unigram |>
  count(word, sort = TRUE) |>
  filter(n > 100) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

## Most Common Word (Graph)

```{r}
unigram |>
  count(word, sort = TRUE) |>
  filter(n > 100) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) 
```

## Most Common Word by Album 

```{r}
#| echo: true
#| eval: false
unigram |> 
  count(album, word, sort = TRUE) |>
  group_by(album) |>
  slice_max(n, n = 10) |>
  mutate(word = reorder_within(word, n, album)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  scale_y_reordered() +
  labs(y = NULL) +
  facet_wrap(~ album, scales = "free_y")
```

## Most Common Word by Album 

```{r}
unigram |> 
  count(album, word, sort = TRUE) |>
  group_by(album) |>
  slice_max(n, n = 10) |>
  mutate(word = reorder_within(word, n, album)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  scale_y_reordered() +
  labs(y = NULL) +
  facet_wrap(~ album, scales = "free_y")
```

## Bigram (Code)

```{r}
bigram <- Data |>
  select(album, doc, lyrics) |>
  mutate(lyrics = tolower(lyrics)) |>
  unnest_tokens(bigram, 
                lyrics, 
                token = "ngrams", 
                n = 2) |>
  separate(
    col = bigram,
    sep = " ",
    into = c("w1", "w2"),
    remove = FALSE
  ) |>
  filter(!w1 %in% stop_words$word) |>
  filter(!w2 %in% stop_words$word) |>
  filter(!w1 %in% song_words$word) |>
  filter(!w2 %in% song_words$word) |>
  filter(!is.na(bigram))
```

```{r}
#| echo: true
#| eval: false
bigram <- Data |>
  select(doc, album, lyrics) |>
  mutate(lyrics = tolower(lyrics)) |>
  unnest_tokens(
    bigram, 
    lyrics, 
    token = "ngrams", 
    n = 2) |>
  separate(
    col = bigram,
    sep = " ",
    into = c("w1", "w2"),
    remove = FALSE
  ) |>
  filter(!w1 %in% stop_words$word) |>
  filter(!w2 %in% stop_words$word) |>
  filter(!w1 %in% song_words$word) |>
  filter(!w2 %in% song_words$word) |>
  filter(!is.na(bigram))
```

## Bigram (Table)

```{r}
reactable(
  bigram,
  wrap = FALSE,
  defaultPageSize = 4,
  defaultColDef = colDef(minWidth = 300))
```

## Most Common Bigram (Code)

```{r}
#| echo: true
#| eval: false
bigram |>
  count(bigram, sort = TRUE) |>
  filter(n > 13) |>
  mutate(bigram = reorder(bigram, n)) |>
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL)
```

## Most Common Bigram (Graph)

```{r}
bigram |>
  count(bigram, sort = TRUE) |>
  filter(n > 13) |>
  mutate(bigram = reorder(bigram, n)) |>
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL)
```

## Most Common Bigram by Album 

```{r}
#| echo: true
#| eval: false
bigram |> 
  count(album, bigram, sort = TRUE) |>
  group_by(album) |>
  slice_max(n, n = 10) |>
  mutate(word = reorder_within(bigram, n, album)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  scale_y_reordered() +
  labs(y = NULL) +
  facet_wrap(~ album, scales = "free_y")
```

## Most Common Bigram by Album 

```{r}
bigram |> 
  count(album, bigram, sort = TRUE) |>
  group_by(album) |>
  slice_max(n, n = 10) |>
  mutate(word = reorder_within(bigram, n, album)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  scale_y_reordered() +
  labs(y = NULL) +
  facet_wrap(~ album, scales = "free_y")
```

## Review

1. What’s the difference between a document, text, and the corpus in this project?

2. Why were stop words and song words removed?

3. Your facet plot is not sorted within each album. What two helpers fix it?

4.  Why does `slice_max(n, n = 10)` differ from `filter(n > 10)` after count()?

## Sentiment Analysis 

{{< video src="https://youtu.be/K-a8s8OLBSE?si=YKbF1sMcnuTm_7Mj" width="100%" height="80%" >}}

## Sentiment Dictionaries

The tidytext package provides access to several sentiment lexicons. 

```{r}
#| warning: false
#| error: false
install.packages("tidytext", repos="http://cran.us.r-project.org")
library("tidytext")
```

```{r}
#| echo: true
#| eval: false
install.packages("tidytext")
library("tidytext")
```


```{r}
# install.packages("textdata", repos="http://cran.us.r-project.org")
# library("textdata")
```

There are three general-purpose lexicons are included wiht tidytext. We will be using two. 

## Lixicons - Bing

**[bing](https://search.r-project.org/CRAN/refmans/textdata/html/lexicon_bing.html)**: the words are assigned scores for positive/negative sentiment. 

```{r}
#| echo: true
#| eval: false
get_sentiments("bing")
```

```{r}
get_sentiments("bing")
```


## Lexicons - Afinn

**[afinn](https://search.r-project.org/CRAN/refmans/oolong/html/afinn.html)**: assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

```{r}
#| echo: true
#| eval: false
get_sentiments("afinn")
```

```{r}
get_sentiments("afinn")
```

## Important Note

These dictionaries were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data. Given this information, we may hesitate to apply these sentiment lexicons to styles of text dramatically different from what they were validated on. [^2]

[^2]: [Text Mining with R - Sentiment analysis with tidy data](https://www.tidytextmining.com/sentiment#the-sentiments-datasets)


## Tidy Albums

```{r}
# tidy_albums <- Data |>
#   group_by(album) |>
#   mutate(
#     track = row_number() 
#   ) |>
#   ungroup() |>
#   unnest_tokens(word, text)
```
## Sentiment Trajectory for each ablum

(Cont)

* sentiment over time by albumn

## Bing Join

```{r}
bing_word_counts <- unigram |>
  inner_join(get_sentiments("bing")) |>
  count(word, sentiment, sort = TRUE) |>
  ungroup()
```

## Most Common +/- Songs


```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

## Word Cloud (code)

(add info about)

```{r}
#| echo: true
#| eval: false
library(wordcloud)
library(RColorBrewer)

unigram %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

## Word Cloud (graph)

```{r}
library(wordcloud)
library(RColorBrewer)

unigram %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

## Negation 

(add info)

## Review



## Conclusion & Resources

Good bye thanks

